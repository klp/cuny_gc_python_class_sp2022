{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50c60b7-53d9-4f00-8c2d-a044d4232d71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Analysis Assignment\n",
    "\n",
    "## Assignment Details\n",
    "\n",
    "The text below is a summary from [this document](https://docs.google.com/document/d/1FNRmAS_vc-2eQESHH6uZMcuDVr720rsx-aDWGH3oz_Y/edit?usp=sharing).\n",
    "\n",
    "The main goal of the assignment is to have you practice the tools we have been using in class.\n",
    "\n",
    "The requirements are:\n",
    "\n",
    "- Choose one or more texts to work with.\n",
    "- Either save the text files in your working directory, or have python get them from a web address.\n",
    "- If needed: convert your text from bytes to a string\n",
    "- Tokenize your text\n",
    "- Make it an NLTK text object so you can use nltk tools on it\n",
    "- Clean the text in a way that is appropriate for the kind of analysis you want to do.\n",
    "- Run some analysis\n",
    "- Report findings\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- If you plan to use functions, have your functions as a separate python file and import it in your main file.\n",
    "\n",
    "- Please, over-comment your script. Make sure to comment every step of the way. Make sure to not only explain what you are doing in terms of code, but also your analytical goal too. For instance, both \"I am running a for loop to remove the common words\" and \"I am trying to see how these two authors compare in terms of the ratio of unusual words to total words\" are kind of comments I want to see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54381b50-b167-446f-a52f-a9dbcef7138f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignment Submission\n",
    "\n",
    "### Itinerary\n",
    "\n",
    "I want to compare usage of modal verbs in American/English men and women authors from 19th century / early 20th literature. I wonder if there is any difference based on nationality or gender. To do my proposed analysis, I will:\n",
    "\n",
    "- Download a sample of raw texts from [Project Gutenberg](https://www.gutenberg.org/) using `urllib`\n",
    "- Convert the texts from bytes to a string\n",
    "- Tokenize the texts\n",
    "- Make it an NLTK text object\n",
    "- Clean the texts, including removing front matter and other empherma from Project Gutenberg texts\n",
    "- Create a list of modal verbs, filter for these works in each text\n",
    "- Perhaps create a conditional frequency distribution on all texts (?) to see if we can establish a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df213cb-fb26-43f8-b3da-ef723c5f41d1",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "In the cell below, I'm importing the libraries/modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f854c78-7e9c-4af8-9ea1-41bc46171f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24b94b9c-492b-44ab-96f0-9307bda69443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kaiprenger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen                    # requesting and opening a file on the internet\n",
    "import nltk                                           # our tool for text analysis\n",
    "nltk.download('punkt')                                # required to run word_tokenized() initially\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from helper_funcs import lowered, first_fifty_k, filter_in_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558354b-d20c-4a56-9dbc-4cd32264467b",
   "metadata": {},
   "source": [
    "### Collecting the texts from Project Gutenberg\n",
    "\n",
    "The first step is to get the raw texts from Project Gutenburg. This work is demonstrated in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e51d09-8732-42a4-9ff9-420a40db8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for each plain text file on PG\n",
    "custom_of_country_url = 'https://www.gutenberg.org/cache/epub/11052/pg11052.txt'\n",
    "little_dorrit_url = 'https://www.gutenberg.org/files/963/963-0.txt'\n",
    "persuasion_url = 'https://www.gutenberg.org/cache/epub/105/pg105.txt'\n",
    "the_awkward_age_url = 'https://www.gutenberg.org/files/7433/7433-0.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b31a5174-2a2b-4d4e-b901-46fcac4db4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open URLs\n",
    "custom_country_file = urlopen(custom_of_country_url)\n",
    "little_dorrit_file = urlopen(little_dorrit_url)\n",
    "persuasion_file = urlopen(persuasion_url)\n",
    "the_awkward_age_file = urlopen(the_awkward_age_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d820d95-ea9d-4b0b-8c05-c395213ac821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the texts and assign them to a variables\n",
    "custom_country_raw = custom_country_file.read()\n",
    "little_dorrit_raw = little_dorrit_file.read()\n",
    "persuasion_raw = persuasion_file.read()\n",
    "the_awkward_age_raw = the_awkward_age_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf98c4f-5bf2-41d0-9925-a2451bb94d0b",
   "metadata": {},
   "source": [
    "### Converting the raw text to strings and lists\n",
    "\n",
    "Below, I demonstrate that while we have the texts available, they're in a format that isn't conducive to text analysis. We will convert these UTF-8 bytes to strings, and finally, to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1f56ba-5b8c-4ff9-af31-9cd3b478f2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type for a given text\n",
    "type(little_dorrit_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4d531-797b-478f-9806-c70439093266",
   "metadata": {},
   "source": [
    "If you were run `little_dorrit_raw` you'll notice when the text is in bytes, you'll get unicode embedded into the text (e.g. `b'\\xef\\xbb\\xbf\\r\\n`). Next, we will decode the raw files into a string to make it more usable for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8ae2d4-1561-4c34-9feb-e364d9b544e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_of_country = custom_country_raw.decode()\n",
    "little_dorrit = little_dorrit_raw.decode()\n",
    "persuasion = persuasion_raw.decode()\n",
    "awkward_age = the_awkward_age_raw.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3674c4f-ae15-4580-a28a-da2d5425216e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type for a given text\n",
    "type(awkward_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b637f11-f361-44b2-b26c-d18820715713",
   "metadata": {},
   "source": [
    "At this point, the entire text is a string, which provides a grain per letter of the text and may not be super useful for a text analysis. Below you'll see slicing into the text by the first 50 characters to see what we mean by not useful.\n",
    "\n",
    "Side note: If you were to run `awkward_age` you would continue to see unicode like `'\\ufeff`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b58597af-4f00-48ed-bd04-05eaf0d46433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of The Awkward Age, b'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f09f5f-a49c-4385-8c94-f96a9b765f0d",
   "metadata": {},
   "source": [
    "Next, we'll tokenize the text, in order to create a list of strings made up of each discrete element separated by spaces (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333eb7fd-066f-4205-8c1a-d2a24db90ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_age_tokens = nltk.word_tokenize(awkward_age)\n",
    "custom_of_country_tokens = nltk.word_tokenize(custom_of_country)\n",
    "little_dorrit_tokens = nltk.word_tokenize(little_dorrit)\n",
    "persuasion_tokens = nltk.word_tokenize(persuasion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21d37d8a-4d38-4369-b711-250e5f147815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'The',\n",
       " 'Awkward',\n",
       " 'Age',\n",
       " ',',\n",
       " 'by',\n",
       " 'Henry',\n",
       " 'James',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost',\n",
       " 'and',\n",
       " 'with',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'restrictions',\n",
       " 'whatsoever',\n",
       " '.',\n",
       " 'You',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'it',\n",
       " ',',\n",
       " 'give',\n",
       " 'it',\n",
       " 'away',\n",
       " 'or',\n",
       " 're-use',\n",
       " 'it',\n",
       " 'under',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'License']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate that the tokens are words\n",
    "awkward_age_tokens[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb5d5d-0ba3-4f68-b273-daba81903415",
   "metadata": {},
   "source": [
    "OK, so this is all really repetitive, so I decided to create a function afterwards called `raw_to_tokens`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b109b0-a012-4555-a935-4589fce90eab",
   "metadata": {},
   "source": [
    "### Prepping the text\n",
    "With these texts coming from Project Gutenberg (PG), we'll need to clean up front matter and post script/licensing that PG provides. I will do this by\n",
    "\n",
    "1. Inspecting the first relevant words of a novel \n",
    "2. Identify what I believe to be the word which is likely to not be a part of the front matter that GP adds\n",
    "3. Find the index of the word above in the word tokenized list for the novel\n",
    "4. Perform two slices to make sure to only include the text between the first sentence and the last sentence[<sup>1</sup>](#fn1), excluding the PG cruft\n",
    "5. Remove non-alphabet characters, and lower the case on words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934de31-ab3b-4f23-8c0f-75792184c626",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b94f26ef-c869-4263-94c5-bc18795760dc",
   "metadata": {},
   "source": [
    "#### Prepping the Awkward Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a259b2d-9280-47b8-93e4-a4e92fadaa5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens.index('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b6a206-b8ff-4ca6-b3c4-32c9d9e13e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'recall', 'with', 'perfect', 'ease']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens[126:131]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e5c65-0b97-4d02-812d-3c2bacdbcabe",
   "metadata": {},
   "source": [
    "We'll start our slice for `awkward_age_tokens` at 126. But we also need end the slice to remove post-novel text and license information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd38daf-a0aa-468d-9f41-e87316b0102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive',\n",
       " 'Foundation',\n",
       " ',',\n",
       " 'how',\n",
       " 'to',\n",
       " 'help',\n",
       " 'produce',\n",
       " 'our',\n",
       " 'new',\n",
       " 'eBooks',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'to',\n",
       " 'subscribe',\n",
       " 'to',\n",
       " 'our',\n",
       " 'email',\n",
       " 'newsletter',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'about',\n",
       " 'new',\n",
       " 'eBooks',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens[-25:]    # Inspecting the end of the text to find this out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db21ef-a92a-4f63-9ce3-b2e61937aae0",
   "metadata": {},
   "source": [
    "I decided to approximate where the novel ends here by looking at the producer name of this text version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49405cad-90fe-4f55-ba39-746d27a125cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens.index('Sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544e2b68-0106-4b04-8f0a-2c61bd7aeb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sobol'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens[118]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66d4e4a-22b1-4f8a-be23-f72ed2c2acd3",
   "metadata": {},
   "source": [
    "The method above doesn't work, because the name is mentioned at the beginning. Let's look for the index from a slice beyond the index above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a89eb21-a0b6-4c77-96e6-6203681d8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_age_wo_pref = awkward_age_tokens[126:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b68ca4a7-36e2-4ba9-aa54-75a69e36a849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181183"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_wo_pref.index('Sobol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9f6edb-53f0-4fbe-a1e3-68c728f4b6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sobol'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_wo_pref[181183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44d48734-fbaa-47d2-a992-fb4d9bb99524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'see',\n",
       " '.',\n",
       " 'There',\n",
       " 'we',\n",
       " 'are',\n",
       " '.',\n",
       " 'Well',\n",
       " ',',\n",
       " '”',\n",
       " 'said',\n",
       " 'Mr.',\n",
       " 'Longdon',\n",
       " '--',\n",
       " '“',\n",
       " 'to-morrow.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_wo_pref[181100:181116]    # trying to nail down the index of the last word of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac63c52-6e9d-4b08-a911-234b0ac61826",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_age_tokens_sliced = awkward_age_wo_pref[:181116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533fdcdb-7a9b-47c1-93a6-81615639af04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'recall', 'with', 'perfect', 'ease', 'the', 'idea']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens_sliced[:7]    # first seven tokens of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8599d15-5cfa-43e9-8094-0b2e9b29c2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well', ',', '”', 'said', 'Mr.', 'Longdon', '--', '“', 'to-morrow.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awkward_age_tokens_sliced[-9:]    # last nine tokens of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105dffcc-f36e-4c0f-8e9b-fea54f41cdfc",
   "metadata": {},
   "source": [
    "#### Prepping Custom of the Country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a87b92-d208-40ba-800a-a9e66d2e9db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_of_country_tokens.index('Undine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f48ac7e9-051c-4909-8e64-689009b9647f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''\", 'Undine', 'Spragg', '--', 'how', 'can', 'you', '?', \"''\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_of_country_tokens[126:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d0e6264-d4ff-4b7f-b621-666ce9678627",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_of_country_token_no_pref = custom_of_country_tokens[126:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a146fffc-fde0-4027-aee1-a92453d24031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166205"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_of_country_token_no_pref.index('Proofreaders')    # in the producer name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b167d686-759d-42f9-81bd-842aabe31a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_of_country_token_no_pref[166129]    # last word in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0391df7e-e827-407c-bc51-6aec78ffb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_of_country_tokens_sliced = custom_of_country_token_no_pref[:166131]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0122e72-3bcb-4d52-a549-1acc228a25a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'her',\n",
       " 'first',\n",
       " 'guests',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'one',\n",
       " 'part',\n",
       " 'she',\n",
       " 'was',\n",
       " 'really',\n",
       " 'made',\n",
       " 'for',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_of_country_tokens_sliced[-20:]    # last twenty items after slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98729d7-802e-4dd0-974c-0594520ef2c5",
   "metadata": {},
   "source": [
    "#### Prepping Little Dorrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72cabfe-5af8-4b77-8dbc-26fe915aaf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_dorrit_tokens.index('occupied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49126e77-9db1-441d-b525-00179aa167e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'occupied',\n",
       " 'with',\n",
       " 'this',\n",
       " 'story',\n",
       " ',',\n",
       " 'during',\n",
       " 'many',\n",
       " 'working',\n",
       " 'hours',\n",
       " 'of',\n",
       " 'two',\n",
       " 'years',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_dorrit_tokens[581:597]    # First sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c66d4a3a-09ef-467e-83c9-478f8642a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_dorrit_tokens_wo_pref = little_dorrit_tokens[581:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4db1cd2b-4da2-4450-8c2e-319971538564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413529"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_dorrit_tokens_wo_pref.index('Widger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c765c364-97a4-41dd-b0bf-fdae53329ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uproar'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_dorrit_tokens_wo_pref[413460]    # last word novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a4dd5c9-db35-4dcc-9e05-446d06f20222",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_dorrit_tokens_sliced = little_dorrit_tokens_wo_pref[:413460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f31412aa-18f3-4136-a4ca-26273fa5a52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'went',\n",
       " 'quietly',\n",
       " 'down',\n",
       " 'into',\n",
       " 'the',\n",
       " 'roaring',\n",
       " 'streets',\n",
       " ',',\n",
       " 'inseparable',\n",
       " 'and',\n",
       " 'blessed',\n",
       " ';',\n",
       " 'and',\n",
       " 'as',\n",
       " 'they',\n",
       " 'passed',\n",
       " 'along',\n",
       " 'in',\n",
       " 'sunshine',\n",
       " 'and',\n",
       " 'shade',\n",
       " ',',\n",
       " 'the',\n",
       " 'noisy',\n",
       " 'and',\n",
       " 'the',\n",
       " 'eager',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'arrogant',\n",
       " 'and',\n",
       " 'the',\n",
       " 'froward',\n",
       " 'and',\n",
       " 'the',\n",
       " 'vain',\n",
       " ',',\n",
       " 'fretted',\n",
       " 'and',\n",
       " 'chafed',\n",
       " ',',\n",
       " 'and',\n",
       " 'made',\n",
       " 'their',\n",
       " 'usual']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_dorrit_tokens_sliced[-47:]    # last sentence confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "475fbb0c-fda0-4c8b-a831-dc3e6c07349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_dorrit_tokens_prep = lowered(little_dorrit_tokens_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf2946-c3f0-4d52-8895-1fc50c0a2bea",
   "metadata": {},
   "source": [
    "#### Prepping Persuasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c853482e-67de-49e2-997d-9c74e0336136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_tokens.index('Elliot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4d6ea48-b97f-4df5-926e-cad96a78a553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir',\n",
       " 'Walter',\n",
       " 'Elliot',\n",
       " ',',\n",
       " 'of',\n",
       " 'Kellynch',\n",
       " 'Hall',\n",
       " ',',\n",
       " 'in',\n",
       " 'Somersetshire',\n",
       " ',',\n",
       " 'was',\n",
       " 'a',\n",
       " 'man',\n",
       " 'who',\n",
       " ',',\n",
       " 'for',\n",
       " 'his',\n",
       " 'own',\n",
       " 'amusement']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_tokens[121:141]    # part of the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26c2d1cd-b8c8-4579-a8d6-574a677bec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_tokens_wo_pref = persuasion_tokens[121:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6644744-0529-4a3a-b93e-5da3c76d7671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', 'Walter', 'Elliot', ',', 'of', 'Kellynch', 'Hall', ',']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_tokens_wo_pref[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "772340c5-3b9c-4a6b-a6d1-fc6ca952a3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97842"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_tokens_wo_pref.index('Finis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "739e56c9-b284-4e16-8046-59326affd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_tokens_sliced = persuasion_tokens_wo_pref[:97842]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "168a2022-b31a-485a-81ec-c625a12d3bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'its',\n",
       " 'domestic',\n",
       " 'virtues',\n",
       " 'than',\n",
       " 'in',\n",
       " 'its',\n",
       " 'national',\n",
       " 'importance',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persuasion_tokens_sliced[-10:]     # part of the last sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c286-2eb9-40c8-b53d-95471b1eee53",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Now that our texts culled of Project Gutenberg boilerplate, we can normalize the text along the following lines:\n",
    "\n",
    "1. Remove non-alphabet characters, as our study focuses on stylistics related to words\n",
    "2. Convert all chracters to lower case\n",
    "3. Take a sample from each text to avoid counts from longer texts distorting the modal verb analysis from shorter texts[<sup>2</sup>](#fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ba96b63-94e3-439f-8aa6-7665ce24e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove non-alphabet characters and convert all words to lower case in one step\n",
    "awkward_age_tokens_prep = lowered(awkward_age_tokens_sliced)\n",
    "custom_of_country_tokens_prep = lowered(custom_of_country_tokens_sliced)\n",
    "little_dorrit_tokens_prep = lowered(little_dorrit_tokens_sliced)\n",
    "persuasion_tokens_prep = lowered(persuasion_tokens_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0a104c8-b437-4b0c-b403-ae72c32e3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check to see which is the shortest test\n",
    "awkward_len = len(awkward_age_tokens_prep)\n",
    "custom_len = len(custom_of_country_tokens_prep)\n",
    "dorrit_len = len(little_dorrit_tokens_sliced)\n",
    "persuasion_len = len(persuasion_tokens_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80179ba8-cea9-4f97-8c3e-6f38c0cefc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Awkward Age is 139732 words long.\n",
      "Custom of the country is 137847 words long.\n",
      "Little Dorrit is 413460 words long.\n",
      "Persuasion is 82937 words long.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Awkward Age is {awkward_len} words long.\")\n",
    "print(f\"Custom of the country is {custom_len} words long.\")\n",
    "print(f\"Little Dorrit is {dorrit_len} words long.\")\n",
    "print(f\"Persuasion is {persuasion_len} words long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa90bf-80df-41d3-9ff3-9a76b743f74b",
   "metadata": {},
   "source": [
    "Alright, let's take a reasonable sample given that Persuasion is the shortest text, and Little Dorrit at minimum for times larger than the other three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d17406dd-2cc2-4b03-8c96-2a371b63907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_age_tokens_norm = first_fifty_k(awkward_age_tokens_prep)\n",
    "custom_of_country_tokens_norm = first_fifty_k(custom_of_country_tokens_prep)\n",
    "little_dorrit_tokens_norm = first_fifty_k(little_dorrit_tokens_prep)\n",
    "persuasion_tokens_norm = first_fifty_k(persuasion_tokens_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3a572-dd27-48fb-abaa-3d3c84276fd5",
   "metadata": {},
   "source": [
    "Let's verify the new normalized lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44c5b398-c484-4bae-a068-a23792743630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(awkward_age_tokens_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6606f-05d0-4095-a807-2241628bd085",
   "metadata": {},
   "source": [
    "### Filter out all non-modal verbs\n",
    "\n",
    "OK, now we'll take those normalized lists of strings/words from each novel, and filter out non-modal verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23a10995-b189-433d-8d08-36909f987a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_verbs = ['can', 'could', 'may', 'might', 'must', 'will', 'would', 'should']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5aee240c-6b6d-45da-a4db-173b240adbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_modals = filter_in_words(awkward_age_tokens_norm, modal_verbs)\n",
    "custom_modals = filter_in_words(custom_of_country_tokens_norm, modal_verbs)\n",
    "dorrit_modals = filter_in_words(little_dorrit_tokens_norm, modal_verbs)\n",
    "persuasion_modals = filter_in_words(persuasion_tokens_norm, modal_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cc275-05f8-45ac-af4f-3817b49f6d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a22560f-4b7f-4440-b3ac-225f48452efb",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "<span id=\"fn1\"> 1. I included author prefaces because we're doing stylistics analysis. </span>\n",
    "\n",
    "<span id=\"fn2\"> 2. Having a percent of total could also help, but this seemed more in keeping with the class material, and I ran out of time to figure out how to drop a list of lists into a Pandas dataframe to do that easily </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae103c6-c962-4040-9c3b-e7ff0631a6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
